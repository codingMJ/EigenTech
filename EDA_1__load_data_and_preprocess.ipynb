{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "transsexual-rugby",
   "metadata": {},
   "source": [
    "### Description:\n",
    "In this notebook, my goal is to upload all data and do some preprocessing <br>\n",
    "\n",
    "In **Stage 1: Import data**:<br>\n",
    "I import all 6 text files into one single dataframe **docs_df** with two columns, <br>\n",
    "where column **Content** has lines of texts and column **FileName** shows the source file name.<br>\n",
    "\n",
    "In **Stage 2: Tokenize files into sentences**:<br>\n",
    "I tokenize each line from column **Content** into sentences and <br>\n",
    "store in new dataframe **docs_df_new** with two columns,<br>\n",
    "where column **Sentence** stores one single tokenized sentence in one row,<br>\n",
    "and column **FileName** stores source file names of each sentence.<br>\n",
    "\n",
    "In **Stage 3: Process sentences**:<br>\n",
    "I do process sentences in column **Sentence** of **docs_df_new**. I do two different processings.<br>\n",
    "First processing is where I lowercase sentence and remove stopwords, <br>\n",
    "this results are stored in new column **Clean sentence**<br>\n",
    "Second processing is where I lowercase sentence and remove stopwords and replace each word to it's lemma, <br>\n",
    "this results are stored in new column **Lemmatized**<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "casual-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-mount",
   "metadata": {},
   "source": [
    "### Stage 1: Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "trained-tennessee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc1.txt', 'doc2.txt', 'doc3.txt', 'doc4.txt', 'doc5.txt', 'doc6.txt']\n"
     ]
    }
   ],
   "source": [
    "# 1\n",
    "# Get file names\n",
    "main_dir  = 'test docs/'\n",
    "file_pathes = glob.glob(main_dir + \"*.txt\")\n",
    "\n",
    "f_names = [fp.split(\"/\")[1] for fp in file_pathes]  # remove main_dir part\n",
    "f_names = sorted(f_names)\n",
    "print(f_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wrong-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1\n",
    "# Import data from files\n",
    "dfs = []\n",
    "for f in f_names:\n",
    "    df = pd.read_csv(main_dir+f, sep=\"\\t\", names=[\"Content\"])      # Import fie (csv,txt, ...)\n",
    "    df['FileName'] = f\n",
    "    dfs.append(df)\n",
    "    \n",
    "# 2.2\n",
    "# Combine data into single frame\n",
    "docs_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# 2.3\n",
    "# Swap columns\n",
    "docs_df=docs_df.reindex(columns=[\"FileName\", \"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "tight-benjamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape: (366, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>Let me begin by saying thanks to all you who'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>We all made this journey for a reason. It's hu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>That's the journey we're on today. But let me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>My work took me to some of Chicago's poorest n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>It was in these neighborhoods that I received ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FileName                                            Content\n",
       "0  doc1.txt  Let me begin by saying thanks to all you who'v...\n",
       "1  doc1.txt  We all made this journey for a reason. It's hu...\n",
       "2  doc1.txt  That's the journey we're on today. But let me ...\n",
       "3  doc1.txt  My work took me to some of Chicago's poorest n...\n",
       "4  doc1.txt  It was in these neighborhoods that I received ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"DF shape:\", docs_df.shape)\n",
    "docs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-produce",
   "metadata": {},
   "source": [
    "### Stage 2: Tokenize files into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chinese-notion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.sentencizer.Sentencizer at 0x11e074e40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe('sentencizer', before=\"parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divided-disclaimer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe\n",
    "columns = ['FileName', 'Sentence']\n",
    "docs_df_new = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "billion-stanford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sentences to dataframe\n",
    "for row in docs_df.iterrows():\n",
    "    sentences = nlp(row[1][\"Content\"]).sents\n",
    "\n",
    "    for s in sentences:\n",
    "        docs_df_new = docs_df_new.append({'FileName': row[1]['FileName'], 'Sentence': s.text}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "israeli-retailer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF shape: (941, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>Let me begin by saying thanks to all you who'v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>We all made this journey for a reason.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>It's humbling, but in my heart I know you didn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>In the face of war, you believe there can be p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>In the face of despair, you believe there can ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FileName                                           Sentence\n",
       "0  doc1.txt  Let me begin by saying thanks to all you who'v...\n",
       "1  doc1.txt             We all made this journey for a reason.\n",
       "2  doc1.txt  It's humbling, but in my heart I know you didn...\n",
       "3  doc1.txt  In the face of war, you believe there can be p...\n",
       "4  doc1.txt  In the face of despair, you believe there can ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"DF shape:\", docs_df_new.shape)\n",
    "docs_df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-machine",
   "metadata": {},
   "source": [
    "### Stage 3: Process sentences: \n",
    "Remove stopwords <br>\n",
    "Take word lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deluxe-collect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def remove_stopwords(sent_):\n",
    "    # 4.2\n",
    "    # Apply model on docs\n",
    "    docs_sent = nlp(sent_)\n",
    "\n",
    "    # 4.3    \n",
    "    # Get stopwords\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    # 4.4\n",
    "    # Take cleaned lemmatized words\n",
    "    proc_tokens = []\n",
    "    for token in docs_sent:\n",
    "        if token.lemma_ not in stopwords and \\\n",
    "        not token.is_punct:\n",
    "            proc_tokens.append(token.text.lower())\n",
    "            \n",
    "    return \" \".join(proc_tokens)\n",
    "\n",
    "def remove_stopwords_and_lemmatize(sent_):\n",
    "    # 4.2\n",
    "    # Apply model on docs\n",
    "    docs_sent = nlp(sent_)\n",
    "\n",
    "    # 4.3    \n",
    "    # Get stopwords\n",
    "    stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "    # 4.4\n",
    "    # Take cleaned lemmatized words\n",
    "    proc_tokens = []\n",
    "    for token in docs_sent:\n",
    "        if token.lemma_ not in stopwords and \\\n",
    "        not token.is_punct:\n",
    "            proc_tokens.append(token.lemma_.lower())\n",
    "            \n",
    "    return \" \".join(proc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "above-dover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(941, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply helper functions\n",
    "docs_df_new[\"Clean sentence\"] = docs_df_new[\"Sentence\"].apply(lambda x: remove_stopwords(x))\n",
    "docs_df_new[\"Lemmatized\"] = docs_df_new[\"Sentence\"].apply(lambda x: remove_stopwords_and_lemmatize(x))\n",
    "docs_df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "mathematical-mechanics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FileName</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Clean sentence</th>\n",
       "      <th>Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>Let me begin by saying thanks to all you who'v...</td>\n",
       "      <td>let me begin thanks traveled far wide brave co...</td>\n",
       "      <td>let I begin thank travel far wide brave cold t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>We all made this journey for a reason.</td>\n",
       "      <td>journey reason</td>\n",
       "      <td>journey reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>It's humbling, but in my heart I know you didn...</td>\n",
       "      <td>humbling heart i know come me came believe cou...</td>\n",
       "      <td>humble heart I know come I come believe country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>In the face of war, you believe there can be p...</td>\n",
       "      <td>face war believe peace</td>\n",
       "      <td>face war believe peace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc1.txt</td>\n",
       "      <td>In the face of despair, you believe there can ...</td>\n",
       "      <td>face despair believe hope</td>\n",
       "      <td>face despair believe hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FileName                                           Sentence  \\\n",
       "0  doc1.txt  Let me begin by saying thanks to all you who'v...   \n",
       "1  doc1.txt             We all made this journey for a reason.   \n",
       "2  doc1.txt  It's humbling, but in my heart I know you didn...   \n",
       "3  doc1.txt  In the face of war, you believe there can be p...   \n",
       "4  doc1.txt  In the face of despair, you believe there can ...   \n",
       "\n",
       "                                      Clean sentence  \\\n",
       "0  let me begin thanks traveled far wide brave co...   \n",
       "1                                     journey reason   \n",
       "2  humbling heart i know come me came believe cou...   \n",
       "3                             face war believe peace   \n",
       "4                          face despair believe hope   \n",
       "\n",
       "                                          Lemmatized  \n",
       "0  let I begin thank travel far wide brave cold t...  \n",
       "1                                     journey reason  \n",
       "2    humble heart I know come I come believe country  \n",
       "3                             face war believe peace  \n",
       "4                          face despair believe hope  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "rough-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export file\n",
    "docs_df_new.to_csv(\"Processed_data.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compound-vegetation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-holly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
